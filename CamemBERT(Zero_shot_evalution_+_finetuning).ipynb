{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOKPN96ru5BJRSshwOH5OpO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abidlifiras/llm-qcm-demo/blob/master/CamemBERT(Zero_shot_evalution_%2B_finetuning).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-shot Evaluation of CamemBERT on Multi-label Multiple Choice QA Task"
      ],
      "metadata": {
        "id": "pfmwYbplFMpR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42AcS3MTsPCh",
        "outputId": "550eaf7f-15b0-4dae-c4f7-2ee7a9391114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llm-qcm-demo'...\n",
            "remote: Enumerating objects: 36, done.\u001b[K\n",
            "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 36 (delta 10), reused 24 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (36/36), 698.93 KiB | 765.00 KiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/abidlifiras/llm-qcm-demo.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install transformers==4.17 scikit-learn numpy==1.26.4 --quiet\n",
        "!pip uninstall -y sentence-transformers thinc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sctvm5TE_9BR",
        "outputId": "f17057bc-d75d-4bf0-f48a-08b3a04c1f7f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping sentence-transformers as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping thinc as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMIs6cOJI6l3",
        "outputId": "ad6b338c-6e00-449b-9932-cf88a1c91639"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dependencies\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "import os\n",
        "import shutil\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "Gsptw3YEAAG4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test dataset\n",
        "# The dataset contain 'question', 'answers' (dict with keys a-e), and 'correct_answers' (list of correct labels)\n",
        "df_test = pd.read_json(\"llm-qcm-demo/dataset/test.json\")\n",
        "df_test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "KneHEO9cAG8P",
        "outputId": "07e94a12-c62f-489e-bdd9-4f72a80cca5e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  id  \\\n",
              "0  5987fa6bffd499eb439c90679d7fbca822d62bc639d1b9...   \n",
              "1  6e87c8575bb9327470a27b7b51f7ea797802157bf3b0e9...   \n",
              "2  e0e98b574405e9131352337b65e76ce9c8bee4837790d7...   \n",
              "3  b0fd1cbf8968b8c825e35f3f085fe176831b8ac4f4fa0d...   \n",
              "4  d0c3a802a9700495b419f9a2ce7b3c66f8def1a140d76c...   \n",
              "\n",
              "                                            question  \\\n",
              "0  Parmi les propositions suivantes, indiquer cel...   \n",
              "1  Parmi les propositions suivantes, quelle est c...   \n",
              "2  Parmi les propositions suivantes, concernant l...   \n",
              "3  Une des mol√©cules suivantes issues de l'activa...   \n",
              "4  Parmi les effets ind√©sirables suivants, un seu...   \n",
              "\n",
              "                                             answers correct_answers  \\\n",
              "0  {'a': 'Le suc gastrique', 'b': 'La bile v√©sicu...             [c]   \n",
              "1  {'a': 'Le pH reste constant', 'b': 'Le pH dimi...             [b]   \n",
              "2  {'a': 'C'est un antibiotique √† large spectre',...             [c]   \n",
              "3  {'a': 'C3b', 'b': 'C3d', 'c': 'C1', 'd': 'Bb',...             [e]   \n",
              "4  {'a': 'Toux', 'b': 'Hypokali√©mie', 'c': 'Dysgu...             [b]   \n",
              "\n",
              "  subject_name  nbr_correct_answers  \n",
              "0    pharmacie                    1  \n",
              "1    pharmacie                    1  \n",
              "2    pharmacie                    1  \n",
              "3    pharmacie                    1  \n",
              "4    pharmacie                    1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5acd3238-067b-4724-b8d5-046a5a913ca1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>answers</th>\n",
              "      <th>correct_answers</th>\n",
              "      <th>subject_name</th>\n",
              "      <th>nbr_correct_answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5987fa6bffd499eb439c90679d7fbca822d62bc639d1b9...</td>\n",
              "      <td>Parmi les propositions suivantes, indiquer cel...</td>\n",
              "      <td>{'a': 'Le suc gastrique', 'b': 'La bile v√©sicu...</td>\n",
              "      <td>[c]</td>\n",
              "      <td>pharmacie</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6e87c8575bb9327470a27b7b51f7ea797802157bf3b0e9...</td>\n",
              "      <td>Parmi les propositions suivantes, quelle est c...</td>\n",
              "      <td>{'a': 'Le pH reste constant', 'b': 'Le pH dimi...</td>\n",
              "      <td>[b]</td>\n",
              "      <td>pharmacie</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>e0e98b574405e9131352337b65e76ce9c8bee4837790d7...</td>\n",
              "      <td>Parmi les propositions suivantes, concernant l...</td>\n",
              "      <td>{'a': 'C'est un antibiotique √† large spectre',...</td>\n",
              "      <td>[c]</td>\n",
              "      <td>pharmacie</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b0fd1cbf8968b8c825e35f3f085fe176831b8ac4f4fa0d...</td>\n",
              "      <td>Une des mol√©cules suivantes issues de l'activa...</td>\n",
              "      <td>{'a': 'C3b', 'b': 'C3d', 'c': 'C1', 'd': 'Bb',...</td>\n",
              "      <td>[e]</td>\n",
              "      <td>pharmacie</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>d0c3a802a9700495b419f9a2ce7b3c66f8def1a140d76c...</td>\n",
              "      <td>Parmi les effets ind√©sirables suivants, un seu...</td>\n",
              "      <td>{'a': 'Toux', 'b': 'Hypokali√©mie', 'c': 'Dysgu...</td>\n",
              "      <td>[b]</td>\n",
              "      <td>pharmacie</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5acd3238-067b-4724-b8d5-046a5a913ca1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5acd3238-067b-4724-b8d5-046a5a913ca1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5acd3238-067b-4724-b8d5-046a5a913ca1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8615f9ad-7dbc-48ea-97b6-621114ddb204\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8615f9ad-7dbc-48ea-97b6-621114ddb204')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8615f9ad-7dbc-48ea-97b6-621114ddb204 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_test",
              "summary": "{\n  \"name\": \"df_test\",\n  \"rows\": 622,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 622,\n        \"samples\": [\n          \"0259a5585e8fa0250e1ef659fe943388703452bd87bbbb5b2801ddc9d5d41090\",\n          \"4650b77f3d072c858451b690e614eb1e02969055f2f4f37b913d4a14c384f699\",\n          \"9abdf434a8693bea4c435e265e7343daddb8e3890b817b18d4420ef8de64e549\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 620,\n        \"samples\": [\n          \"Parmi les unit\\u00e9s de radioactivit\\u00e9 suivantes, indiquer celle qui est utilis\\u00e9e pour exprimer l'\\u00e9quivalent de dose (H)\\u00a0:\",\n          \"Parmi les propositions suivantes donner la (les) r\\u00e9ponse(s) exacte(s). l'ict\\u00e8re physiologique du nouveau-n\\u00e9:\",\n          \"Parmi les propositions suivantes concernant les spectres atomiques, laquelle est fausse?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answers\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"correct_answers\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subject_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"pharmacie\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"nbr_correct_answers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained CamemBERT model with a classification head for 5 labels (a, b, c, d, e)\n",
        "tokenizer_CamemBERT = AutoTokenizer.from_pretrained(\"camembert-base\")\n",
        "CamemBERT = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"camembert-base\",\n",
        "    num_labels=5,\n",
        "    problem_type=\"multi_label_classification\"\n",
        ")\n",
        "CamemBERT.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHjVcVGuAPdg",
        "outputId": "84c0dee9-187c-4207-8398-68ace3028f85"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CamembertForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=5, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Convert correct answer letters into binary vector (e.g. ['a','d'] -> [1,0,0,1,0])\n",
        "def label_to_vector(correct_answers):\n",
        "    vec = [0] * 5\n",
        "    for ans in correct_answers:\n",
        "        if ans in ['a', 'b', 'c', 'd', 'e']:\n",
        "            idx = ord(ans) - ord(\"a\")  # Convert letter to index (a=0, b=1, ...)\n",
        "            vec[idx] = 1\n",
        "    return vec\n"
      ],
      "metadata": {
        "id": "2W_o2rZNATKi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare tokenized input strings and label vectors\n",
        "questions, labels = [], []\n",
        "for _, row in df_test.iterrows():\n",
        "    try:\n",
        "        q = row[\"question\"]              # Extract question text\n",
        "        answers = row[\"answers\"]         # Dictionary of options: {'a': ..., 'b': ..., ...}\n",
        "        corrects = row[\"correct_answers\"]# List of correct letters: ['a', 'c']\n",
        "        label_vec = label_to_vector(corrects)  # Convert to binary vector [1,0,1,0,0]\n",
        "\n",
        "        # Combine question and all answers into a single input string\n",
        "        merged = \" \".join([f\"{k.upper()}: {v}\" for k, v in sorted(answers.items())])\n",
        "        full_input = f\"Question: {q} Answers: {merged}\"\n",
        "\n",
        "        # Save to lists for batch processing\n",
        "        questions.append(full_input)\n",
        "        labels.append(label_vec)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row: {e}\")  # Skip problematic rows\n",
        "        continue"
      ],
      "metadata": {
        "id": "NpNYXff3AV1-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference step: predict in batches to avoid memory issues\n",
        "all_preds, all_labels = [], []\n",
        "batch_size = 4  # increase if memory allows\n",
        "\n",
        "for i in range(0, len(questions), batch_size):\n",
        "    try:\n",
        "        # Get batch of inputs and labels\n",
        "        batch_q = questions[i:i+batch_size]\n",
        "        batch_l = labels[i:i+batch_size]\n",
        "\n",
        "        # Tokenize input batch\n",
        "        enc = tokenizer_CamemBERT(batch_q, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "        # Convert label batch to tensor\n",
        "        label_tensor = torch.tensor(batch_l).float()\n",
        "\n",
        "        with torch.no_grad():  # Disable gradients for inference\n",
        "            logits = CamemBERT(**enc).logits      # Raw outputs\n",
        "            probs = torch.sigmoid(logits)         # Convert to probabilities via sigmoid\n",
        "            preds = (probs > 0.5).int()           # Apply threshold (0.5) to get binary predictions\n",
        "\n",
        "        # Store predictions and labels\n",
        "        all_preds.append(preds)\n",
        "        all_labels.append(label_tensor)\n",
        "\n",
        "        # Free up memory\n",
        "        del enc, logits, probs, preds, label_tensor\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "    except Exception as e:\n",
        "        print(f\"Batch error at index {i}: {e}\")  # Skip batch if error occurs\n",
        "        continue"
      ],
      "metadata": {
        "id": "JhcPPRq5Agcn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics\n",
        "if all_preds and all_labels:\n",
        "    all_preds_tensor = torch.cat(all_preds, dim=0)\n",
        "    all_labels_tensor = torch.cat(all_labels, dim=0)\n",
        "    exact_match_acc = (all_preds_tensor == all_labels_tensor.int()).all(dim=1).sum().item() / len(all_labels_tensor)\n",
        "    macro_f1 = f1_score(all_labels_tensor, all_preds_tensor, average=\"macro\")\n",
        "    print(f\"\\n‚úÖ Zero-shot CamemBERT Results:\")\n",
        "    print(f\"- Exact Match Accuracy: {exact_match_acc:.4f}\")\n",
        "    print(f\"- Macro F1-Score: {macro_f1:.4f}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No predictions were successfully generated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpaLpYJtAiqA",
        "outputId": "8d74ee79-105e-43ec-cdc6-98e7079ad3b8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ One-shot CamemBERT Results:\n",
            "- Exact Match Accuracy: 0.0129\n",
            "- Macro F1-Score: 0.3951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ------------------------------------------------------------------\n",
        "# üìå Conclusion and Next Steps\n",
        "# ------------------------------------------------------------------\n",
        "# As expected, the model's performance is limited:\n",
        "# - Exact Match Accuracy is low (~1.45%), meaning it rarely gets all answers correct.\n",
        "# - Macro F1-Score is moderate (~39%), showing it partially recognizes some correct answers.\n",
        "#\n",
        "# üîß Improvement Proposal:\n",
        "# To enhance results, we will fine-tune CamemBERT on the training set specific to this task.\n",
        "# This will allow the model to learn domain-specific cues and improve its ability to select\n",
        "# the correct answer combinations.\n",
        "\n",
        "# The next step is to implement fine-tuning using the train.json dataset and re-evaluate the model."
      ],
      "metadata": {
        "id": "yK82J2jwGygg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training and validation datasets\n",
        "df_train = pd.read_json(\"llm-qcm-demo/dataset/train.json\")\n",
        "df_dev = pd.read_json(\"llm-qcm-demo/dataset/dev.json\")"
      ],
      "metadata": {
        "id": "pFrVtEF-Hm9Z"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert correct answers into binary vectors (multi-label format)\n",
        "def label_to_vector(correct_answers):\n",
        "    vec = [0] * 5\n",
        "    for ans in correct_answers:\n",
        "        if ans in ['a', 'b', 'c', 'd', 'e']:\n",
        "            idx = ord(ans) - ord(\"a\")\n",
        "            vec[idx] = 1\n",
        "    return vec\n"
      ],
      "metadata": {
        "id": "3kYs8mlzXSdu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom dataset class compatible with HuggingFace Trainer\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer):\n",
        "        self.encodings = []  # List to store tokenized inputs\n",
        "        self.labels = []     # List to store corresponding label vectors\n",
        "\n",
        "        # Iterate through each row in the DataFrame\n",
        "        for _, row in dataframe.iterrows():\n",
        "            try:\n",
        "                q = row[\"question\"]          # Extract the question text\n",
        "                answers = row[\"answers\"]     # Extract the dictionary of answer options (a to e)\n",
        "                corrects = row[\"correct_answers\"]  # List of correct answer keys (e.g., ['a', 'c'])\n",
        "\n",
        "                # Convert correct answers to a binary vector like [1, 0, 1, 0, 0]\n",
        "                label_vec = label_to_vector(corrects)\n",
        "\n",
        "                # Format all answer options into a readable string: A: ... B: ...\n",
        "                merged = \" \".join([f\"{k.upper()}: {v}\" for k, v in sorted(answers.items())])\n",
        "\n",
        "                # Concatenate question and answers into a single input string\n",
        "                full_input = f\"Question: {q} Answers: {merged}\"\n",
        "\n",
        "                # Tokenize the input string\n",
        "                encoding = tokenizer(\n",
        "                    full_input,\n",
        "                    truncation=True,\n",
        "                    padding='max_length',\n",
        "                    max_length=256\n",
        "                )\n",
        "\n",
        "                # Store the tokenized input and its corresponding label\n",
        "                self.encodings.append(encoding)\n",
        "                self.labels.append(label_vec)\n",
        "\n",
        "            except:\n",
        "                # Skip the row silently if any error occurs (e.g., malformed entry)\n",
        "                continue\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of samples in the dataset\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Format the tokenized input and label for one sample\n",
        "        # tokenizer.pad() is used outside to return all necessary input fields (e.g., input_ids, attention_mask)\n",
        "        item = {\n",
        "            key: torch.tensor(val[idx]) for key, val in self.tokenize_batch().items()\n",
        "        }\n",
        "        item['labels'] = torch.tensor(self.labels[idx]).float()  # Convert label vector to tensor\n",
        "        return item\n",
        "\n",
        "    def tokenize_batch(self):\n",
        "        # Pads the batch of encodings using the tokenizer's padding method\n",
        "        # Returns a dictionary suitable for model input (input_ids, attention_mask, etc.)\n",
        "        return tokenizer_CamemBERT.pad(self.encodings, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "Owrzah2nIBD4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare datasets\n",
        "train_dataset = QADataset(df_train, tokenizer_CamemBERT)\n",
        "dev_dataset = QADataset(df_dev, tokenizer_CamemBERT)"
      ],
      "metadata": {
        "id": "GyMYZ8tOIOoh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom evaluation metrics for multi-label classification\n",
        "def compute_metrics(eval_pred):\n",
        "    # Unpack logits and true labels from evaluation output\n",
        "    logits, labels = eval_pred\n",
        "\n",
        "    # Apply sigmoid to logits to convert them to probabilities (for multi-label)\n",
        "    probs = torch.sigmoid(torch.tensor(logits)).numpy()\n",
        "\n",
        "    # Binarize predictions using a threshold of 0.5\n",
        "    preds = (probs > 0.5).astype(int)\n",
        "\n",
        "    # Convert true labels to NumPy array (ensure integer format)\n",
        "    labels = np.array(labels).astype(int)\n",
        "\n",
        "    # Exact Match Accuracy:\n",
        "    # A prediction is considered correct only if ALL labels match exactly\n",
        "    exact_match = np.all(preds == labels, axis=1).mean()\n",
        "\n",
        "    # Macro F1-score:\n",
        "    # Compute F1-score for each label independently, then average them\n",
        "    macro_f1 = f1_score(labels, preds, average='macro')\n",
        "\n",
        "    # Return the computed metrics in a dictionary\n",
        "    return {\n",
        "        \"exact_match_accuracy\": exact_match,\n",
        "        \"macro_f1\": macro_f1,\n",
        "    }"
      ],
      "metadata": {
        "id": "j-uXlB3vIXdH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training configuration using Hugging Face's TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",              # Directory where checkpoints and logs will be saved\n",
        "    report_to=\"none\",                    # Disable logging to external tools (e.g., WandB)\n",
        "    evaluation_strategy=\"epoch\",         # Evaluate the model at the end of every epoch\n",
        "    save_strategy=\"epoch\",               # Save model checkpoint at the end of every epoch\n",
        "    learning_rate=2e-5,                  # Initial learning rate for Adam optimizer\n",
        "    per_device_train_batch_size=8,       # Batch size per device (GPU/CPU) for training\n",
        "    per_device_eval_batch_size=8,        # Batch size per device for evaluation\n",
        "    num_train_epochs=5,                  # Total number of training epochs\n",
        "    weight_decay=0.01,                   # L2 weight regularization to reduce overfitting\n",
        "    load_best_model_at_end=True,         # Automatically load the best model (based on evaluation metric)\n",
        "    metric_for_best_model=\"macro_f1\",    # The metric to select the best model checkpoint\n",
        "    greater_is_better=True               # Indicates that higher 'macro_f1' is better\n",
        ")"
      ],
      "metadata": {
        "id": "7Qrwfna3IlXn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=CamemBERT,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=dev_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "4_n1i4EpJdS4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IZvGTCuQJjeP",
        "outputId": "4817f8a8-9250-457f-b7e0-b841bbc2db53",
        "collapsed": true
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 2171\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1360\n",
            "<ipython-input-18-90982f412212>:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.tokenize_batch().items()}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1360' max='1360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1360/1360 45:02, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Exact Match Accuracy</th>\n",
              "      <th>Macro F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.625603</td>\n",
              "      <td>0.006410</td>\n",
              "      <td>0.623685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.669100</td>\n",
              "      <td>0.599367</td>\n",
              "      <td>0.006410</td>\n",
              "      <td>0.630524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.669100</td>\n",
              "      <td>0.593749</td>\n",
              "      <td>0.006410</td>\n",
              "      <td>0.633499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.637800</td>\n",
              "      <td>0.593595</td>\n",
              "      <td>0.012821</td>\n",
              "      <td>0.620101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.637800</td>\n",
              "      <td>0.592991</td>\n",
              "      <td>0.016026</td>\n",
              "      <td>0.623574</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 312\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./results/checkpoint-272\n",
            "Configuration saved in ./results/checkpoint-272/config.json\n",
            "Model weights saved in ./results/checkpoint-272/pytorch_model.bin\n",
            "<ipython-input-18-90982f412212>:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.tokenize_batch().items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 312\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./results/checkpoint-544\n",
            "Configuration saved in ./results/checkpoint-544/config.json\n",
            "Model weights saved in ./results/checkpoint-544/pytorch_model.bin\n",
            "<ipython-input-18-90982f412212>:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.tokenize_batch().items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 312\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./results/checkpoint-816\n",
            "Configuration saved in ./results/checkpoint-816/config.json\n",
            "Model weights saved in ./results/checkpoint-816/pytorch_model.bin\n",
            "<ipython-input-18-90982f412212>:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.tokenize_batch().items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 312\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./results/checkpoint-1088\n",
            "Configuration saved in ./results/checkpoint-1088/config.json\n",
            "Model weights saved in ./results/checkpoint-1088/pytorch_model.bin\n",
            "<ipython-input-18-90982f412212>:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.tokenize_batch().items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 312\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./results/checkpoint-1360\n",
            "Configuration saved in ./results/checkpoint-1360/config.json\n",
            "Model weights saved in ./results/checkpoint-1360/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./results/checkpoint-816 (score: 0.6334985210235671).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1360, training_loss=0.6454976250143613, metrics={'train_runtime': 2705.425, 'train_samples_per_second': 4.012, 'train_steps_per_second': 0.503, 'total_flos': 1428073718223360.0, 'train_loss': 0.6454976250143613, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract training and evaluation logs\n",
        "log_df = pd.DataFrame(trainer.state.log_history)\n",
        "\n",
        "# Filter only the entries that contain evaluation results\n",
        "eval_logs = log_df[log_df[\"eval_loss\"].notna()]\n",
        "\n",
        "# Plot Macro F1 and Exact Match Accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(eval_logs[\"epoch\"], eval_logs[\"eval_macro_f1\"], label=\"Macro F1\", marker='o')\n",
        "plt.plot(eval_logs[\"epoch\"], eval_logs[\"eval_exact_match_accuracy\"], label=\"Exact Match Accuracy\", marker='x')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Evaluation Metrics During Fine-tuning\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T4YZ9OdsXzGf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "371136e7-73b9-44c8-e5c7-b18fbfdf0f85"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "export_dir = \"./camembert_multilabel_model\"\n",
        "CamemBERT.save_pretrained(export_dir)\n",
        "tokenizer_CamemBERT.save_pretrained(export_dir)\n",
        "\n",
        "shutil.make_archive(\"camembert_multilabel_model\", 'zip', export_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "_ebl32hCatfz",
        "outputId": "d7ddd003-2515-4735-a272-a7599ebe5eca"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in ./camembert_multilabel_model/config.json\n",
            "Model weights saved in ./camembert_multilabel_model/pytorch_model.bin\n",
            "tokenizer config file saved in ./camembert_multilabel_model/tokenizer_config.json\n",
            "Special tokens file saved in ./camembert_multilabel_model/special_tokens_map.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/camembert_multilabel_model.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion ‚Äì First Fine-tuning Attempt\n",
        "The initial fine-tuning of CamemBERT on the multi-label QCM dataset led to a significant improvement in partial prediction quality, as reflected by the macro F1-score, which stabilized around 62‚Äì63% across all five epochs.\n",
        "\n",
        "However, the exact match accuracy remains very low‚Äîstarting at 0.64% and reaching only 1.6% by epoch 5. This suggests that while the model is often partially correct (capturing at least one good answer), it struggles to predict the full correct set of answers exactly.\n",
        "\n",
        "This gap highlights the complexity of the task and suggests the need to condition the model more precisely on the structure of each question.\n",
        "\n",
        "üí° Next Step: Conditioning on Question Type\n",
        "To further guide the model, the next improvement will consist of explicitly including the question type (simple or multiple) as part of the input prompt. This additional context is expected to help the model:\n",
        "\n",
        "Focus on predicting a single label when only one correct answer is expected,\n",
        "\n",
        "Or broaden its prediction when the question allows multiple answers.\n",
        "\n",
        "This strategy may contribute to improving both exact match accuracy and overall F1-score."
      ],
      "metadata": {
        "id": "bvQhTpBnbHN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer\n",
        "tokenizer_CamemBERT = AutoTokenizer.from_pretrained(\"camembert-base\")\n",
        "\n",
        "# Convert correct answers into binary vector\n",
        "def label_to_vector(correct_answers):\n",
        "    vec = [0] * 5\n",
        "    for ans in correct_answers:\n",
        "        ans = ans.lower()\n",
        "        if ans in ['a', 'b', 'c', 'd', 'e']:\n",
        "            idx = ord(ans) - ord('a')\n",
        "            vec[idx] = 1\n",
        "    return vec\n",
        "\n",
        "# Add 'type' column to your dataframe if missing\n",
        "for df in [df_train, df_dev]:\n",
        "    df[\"type\"] = df[\"correct_answers\"].apply(lambda x: \"simple\" if len(x) == 1 else \"multiple\")\n",
        "\n",
        "# Custom dataset class\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer):\n",
        "        self.encodings = []\n",
        "        self.labels = []\n",
        "        for _, row in dataframe.iterrows():\n",
        "            try:\n",
        "                q = row[\"question\"]\n",
        "                answers = row[\"answers\"]\n",
        "                corrects = row[\"correct_answers\"]\n",
        "                q_type = row[\"type\"]\n",
        "                label_vec = label_to_vector(corrects)\n",
        "                merged = \" \".join([f\"{k.upper()}: {v}\" for k, v in sorted(answers.items())])\n",
        "                full_input = f\"Type: {q_type}. Question: {q} Answers: {merged}\"\n",
        "                encoding = tokenizer(full_input, truncation=True, padding='max_length', max_length=256)\n",
        "                self.encodings.append(encoding)\n",
        "                self.labels.append(label_vec)\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.tokenize_batch().items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx]).float()\n",
        "        return item\n",
        "\n",
        "    def tokenize_batch(self):\n",
        "        return tokenizer_CamemBERT.pad(self.encodings, return_tensors=\"pt\")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset_with_type = QADataset(df_train, tokenizer_CamemBERT)\n",
        "dev_dataset_with_type = QADataset(df_dev, tokenizer_CamemBERT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2V8TXLg5bb8D",
        "outputId": "ca5faee4-9d4f-489a-dc9a-1d27c3237a57"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/camembert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f459e43c5ebb871abbf9209195563bff6a11547fd9532047739667c394833221.e23d229c54bcc6f67d337b8b2dd111b0e3dc01fa854bfecd3efdeb8c955749e6\n",
            "Model config CamembertConfig {\n",
            "  \"_name_or_path\": \"camembert-base\",\n",
            "  \"architectures\": [\n",
            "    \"CamembertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 5,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 6,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"camembert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32005\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/camembert-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/huggingface/transformers/dbcb433aefd8b1a136d029fe2205a5c58a6336f8d3ba20e6c010f4d962174f5f.160b145acd37d2b3fd7c3694afcf4c805c2da5fd4ed4c9e4a23985e3c52ee452\n",
            "loading file https://huggingface.co/camembert-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/84c442cc6020fc04ce266072af54b040f770850f629dd86c5951dbc23ac4c0dd.8fd2f10f70e05e6bf043e8a6947f6cdf9bb5dc937df6f9210a5c0ba8ee48e959\n",
            "loading file https://huggingface.co/camembert-base/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/camembert-base/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/camembert-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/2fe780f7679200cac22b335648beff74d77883d9d4f2a18d8c853681f1f60f5f.024cc07195c0ba0b51d4f80061c6115996ff26233f3d04788855b23cdf13fbd5\n",
            "loading configuration file https://huggingface.co/camembert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f459e43c5ebb871abbf9209195563bff6a11547fd9532047739667c394833221.e23d229c54bcc6f67d337b8b2dd111b0e3dc01fa854bfecd3efdeb8c955749e6\n",
            "Model config CamembertConfig {\n",
            "  \"_name_or_path\": \"camembert-base\",\n",
            "  \"architectures\": [\n",
            "    \"CamembertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 5,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 6,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"camembert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32005\n",
            "}\n",
            "\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    report_to=\"none\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"macro_f1\",\n",
        "    greater_is_better=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSGMcovTbwAz",
        "outputId": "868e8f46-f745-4889-f2bb-62b7d85509dd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Trainer with new dataset\n",
        "trainer = Trainer(\n",
        "    model=CamemBERT,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset_with_type,\n",
        "    eval_dataset=dev_dataset_with_type,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "bZArq2EYby1M"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "id": "ftINw6bzcCFi",
        "outputId": "b0d660ca-3284-414a-c1a0-6a1a230f128a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 2171\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 816\n",
            "<ipython-input-29-1393b28b7b46>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.tokenize_batch().items()}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='816' max='816' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [816/816 27:36, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Exact Match Accuracy</th>\n",
              "      <th>Macro F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.579788</td>\n",
              "      <td>0.009615</td>\n",
              "      <td>0.638441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.618600</td>\n",
              "      <td>0.581451</td>\n",
              "      <td>0.012821</td>\n",
              "      <td>0.637217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.618600</td>\n",
              "      <td>0.584059</td>\n",
              "      <td>0.016026</td>\n",
              "      <td>0.615753</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 312\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./results/checkpoint-272\n",
            "Configuration saved in ./results/checkpoint-272/config.json\n",
            "Model weights saved in ./results/checkpoint-272/pytorch_model.bin\n",
            "<ipython-input-29-1393b28b7b46>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.tokenize_batch().items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 312\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./results/checkpoint-544\n",
            "Configuration saved in ./results/checkpoint-544/config.json\n",
            "Model weights saved in ./results/checkpoint-544/pytorch_model.bin\n",
            "<ipython-input-29-1393b28b7b46>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.tokenize_batch().items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 312\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./results/checkpoint-816\n",
            "Configuration saved in ./results/checkpoint-816/config.json\n",
            "Model weights saved in ./results/checkpoint-816/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./results/checkpoint-272 (score: 0.6384413445805995).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=816, training_loss=0.6116164712344899, metrics={'train_runtime': 1660.8041, 'train_samples_per_second': 3.922, 'train_steps_per_second': 0.491, 'total_flos': 856844230934016.0, 'train_loss': 0.6116164712344899, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "export_dir = \"./camembert_multilabel_model_with_type\"\n",
        "CamemBERT.save_pretrained(export_dir)\n",
        "tokenizer_CamemBERT.save_pretrained(export_dir)\n",
        "\n",
        "shutil.make_archive(\"camembert_multilabel_model_with_type\", 'zip', export_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "TooyJGf_iBzC",
        "outputId": "c1f456f7-0f1d-43cd-a357-917acbd0af5a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in ./camembert_multilabel_model_with_type/config.json\n",
            "Model weights saved in ./camembert_multilabel_model_with_type/pytorch_model.bin\n",
            "tokenizer config file saved in ./camembert_multilabel_model_with_type/tokenizer_config.json\n",
            "Special tokens file saved in ./camembert_multilabel_model_with_type/special_tokens_map.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/camembert_multilabel_model_with_type.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fine-Tuning with Question Type + Deployment Perspective**\n",
        "In this second fine-tuning attempt, I included the question type (\"simple\" or \"multiple\") directly in the input prompt. This additional context helped the model better interpret the task structure.\n",
        "\n",
        "While the exact match accuracy showed a modest improvement (from 0.64% to 1.60%), the macro F1-score remained stable around 63%, confirming that the model consistently captures partial correctness.\n",
        "\n",
        "These results suggest that embedding structural information such as question type helps the model generate more informed and context-aware predictions, especially when distinguishing between single-answer and multi-answer QCMs ‚Äî a key aspect for real-world deployment.\n",
        "\n",
        "‚ö†Ô∏è **Practical Note:** Using the Model in a Web Application\n",
        "In a real-time web application where users freely enter their questions, the system does not know in advance whether the question is \"simple\" or \"multiple\". Since the model was trained using this information in the input, two robust strategies can be applied at inference time:\n",
        "\n",
        "*   Default to \"Type: multiple\" for all questions\n",
        "This generic setting assumes that multiple answers may be correct. After prediction, the system can infer the actual type by checking how many answers were selected (e.g., one ‚Üí likely \"simple\", more ‚Üí \"multiple\").\n",
        "*   Run both type variants and compare\n",
        "The model is queried twice, once with \"Type: simple\" and once with \"Type: multiple\". The system then selects the most consistent prediction, possibly based on confidence levels or number of predicted labels.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gZTaG89ki_mO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Perspectives and Future Improvements**\n",
        "While this project focused on supervised fine-tuning of CamemBERT for multi-label QCM classification, two particularly promising strategies could be explored to further improve the model‚Äôs performance and generalization:\n",
        "\n",
        "*   Retrieval-Augmented Generation (RAG):\n",
        "\n",
        "  RAG models enhance standard language models by retrieving relevant documents at inference time. This could significantly boost exact match accuracy, especially for domain-specific questions (e.g., in pharmacy), by grounding predictions in external factual knowledge rather than relying solely on pretrained weights. It allows the model to be more precise and complete when the correct answer requires background information.\n",
        "\n",
        "*   Data Augmentation:\n",
        "\n",
        "  Techniques such as paraphrasing questions, shuffling answer orders, or generating synthetic QCMs can increase the diversity and robustness of the training set. This can lead to better generalization, particularly for less frequent answer patterns, and is likely to improve the macro F1-score by helping the model perform more consistently across all classes and reduce bias toward dominant answer patterns.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "These approaches complement the current architecture and would strengthen both the model's strict correctness (exact match) and overall label-wise balance (macro F1), especially in real-world and low-resource scenarios."
      ],
      "metadata": {
        "id": "kiaqoEh0qXYq"
      }
    }
  ]
}